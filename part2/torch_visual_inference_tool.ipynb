{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "import queue\n",
    "import random\n",
    "import time\n",
    "from importlib import reload\n",
    "from multiprocessing import Process, Manager\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.nn import Module\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s', level=logging.INFO, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Async Processes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class GPUMonitor(Process):\n",
    "    def __init__(self, delay):\n",
    "        super(GPUMonitor, self).__init__()\n",
    "        self.delay = delay\n",
    "        self.power_readings = Manager().list()\n",
    "        self.running = True\n",
    "        self.start()\n",
    "\n",
    "    def run(self):\n",
    "        while self.running:\n",
    "            try:\n",
    "                p = Popen('nvidia-smi --query-gpu=power.draw --format=csv,noheader,nounits'.split(' '), stdout=PIPE)\n",
    "                stdout, stderror = p.communicate()\n",
    "                self.power_readings.append(float(stdout.strip()))\n",
    "                p.terminate()\n",
    "            except:\n",
    "                logging.error('Something went wrong while retrieving GPU readings...')\n",
    "            time.sleep(self.delay)\n",
    "\n",
    "    def reset_energy(self):\n",
    "        self.power_readings[:] = []\n",
    "\n",
    "    def get_power_average(self):\n",
    "        return np.mean(self.power_readings)\n",
    "\n",
    "\n",
    "class RequestQueue(Process):\n",
    "    def __init__(self, id, frequency, nr_of_requests):\n",
    "        super(Process, self).__init__()\n",
    "        self.id = id\n",
    "        self.frequency = frequency\n",
    "        self.nr_of_requests = nr_of_requests\n",
    "        self.queue = Manager().Queue(nr_of_requests)\n",
    "        self.start()\n",
    "\n",
    "    def run(self):\n",
    "        logging.info(\"Started simulation with id: {}\".format(self.id))\n",
    "        while self.nr_of_requests > 0:\n",
    "            self.queue.put(Image.open('img/dog.jpg'))\n",
    "            self.nr_of_requests -= 1\n",
    "            time.sleep(1 / self.frequency * random.uniform(0.8, 1.2))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )])\n",
    "with open('image_net_classes.txt') as file:\n",
    "    classes = [line.strip().split(', ')[1] for line in file.readlines()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def infer(model: Module, images, use_gpu=True, verbose=False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if use_gpu:\n",
    "            model.cuda()\n",
    "        images_t = [transform(im) for im in images]\n",
    "        batch = torch.cat([tensor for tensor in [torch.unsqueeze(im_t, 0) for im_t in images_t]])\n",
    "        if use_gpu:\n",
    "            out = model(batch.cuda())\n",
    "        else:\n",
    "            out = model(batch)\n",
    "\n",
    "    for prediction in out:\n",
    "        prediction = prediction.cpu()\n",
    "        _, indices = torch.sort(prediction, descending=True)\n",
    "        percentages = [(torch.nn.functional.softmax(prediction, dim=0)[class_index] * 100).item() for class_index in\n",
    "                       indices[:5]]\n",
    "        if verbose:\n",
    "            logging.info(f'Rank\\tInferred class\\tProbability(%)')\n",
    "            for idx, class_index in enumerate(indices[:5]):\n",
    "                logging.info(f'#{idx}\\t\\t{classes[class_index]}\\t{percentages[idx]}')\n",
    "            logging.info('-----------------------------------------')\n",
    "\n",
    "\n",
    "def run_experiment(model_, input_images_):\n",
    "    t_0 = time.perf_counter()\n",
    "    infer(model_, input_images_, use_gpu=True)\n",
    "    return time.perf_counter() - t_0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:21:33 INFO: Started simulation with id: inference_simulation\n",
      "02:21:48 INFO: Batch processed | t=0.13576744399961171\n",
      "02:22:03 INFO: Batch processed | t=0.08052059100009501\n",
      "02:22:20 INFO: Batch processed | t=0.06266189399957511\n",
      "02:22:36 INFO: Batch processed | t=0.09271269900000334\n",
      "02:22:39 INFO: Average Power: 53.314872521246464 over a duration of 63.09854310500032 seconds\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "frequency = 1\n",
    "nr_of_requests = 64\n",
    "epsilon = 2 / frequency + 0.5\n",
    "model = models.alexnet(pretrained=True)\n",
    "gpu_monitor = GPUMonitor(0.1)\n",
    "rq = RequestQueue('inference_simulation', frequency, nr_of_requests)\n",
    "t_0 = time.perf_counter()\n",
    "while True:\n",
    "    try:\n",
    "        batch = [rq.queue.get(block=True, timeout=epsilon) for _ in range(batch_size)]\n",
    "        t = run_experiment(model, batch)\n",
    "        logging.info(f\"Batch processed | {t=}\")\n",
    "    except queue.Empty:\n",
    "        break\n",
    "\n",
    "logging.info(f'Average Power: {gpu_monitor.get_power_average()} over a duration of {time.perf_counter() - t_0 - epsilon} seconds')\n",
    "rq.terminate()\n",
    "gpu_monitor.terminate()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}