{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from importlib import reload\n",
    "from itertools import product\n",
    "from warnings import catch_warnings, simplefilter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import qmc, norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import *\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s', level=logging.INFO, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network Classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [],
   "source": [
    "class SimpleCNN(Module):\n",
    "    def __init__(self, config, verbose):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        img_dim = config['shape'][2]\n",
    "        channels = config['shape'][1]\n",
    "        conv_layers = np.array([\n",
    "            [LazyConv2d(channels * 3 ** (depth + 1), kernel_size=3, stride=1, padding=1),\n",
    "             LazyBatchNorm2d(),\n",
    "             ReLU(inplace=True),\n",
    "             MaxPool2d(kernel_size=2, stride=2)]\n",
    "            for depth in range(config['factors'].get('nr_conv_layers', 2))\n",
    "        ]).flatten().tolist()\n",
    "        linear_layers = [\n",
    "            LazyLinear(max(math.floor(channels * img_dim ** 2 / 2 ** (depth + 1)), channels)) for depth in\n",
    "            range(config['factors'].get('nr_linear_layers', 3) - 1)\n",
    "        ]\n",
    "        self.layers = Sequential(\n",
    "            *conv_layers,\n",
    "            Flatten(start_dim=1),\n",
    "            *linear_layers,\n",
    "            LazyLinear(config['labels'])\n",
    "        )\n",
    "        if verbose:\n",
    "            logging.info(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DensePolyNN(Module):\n",
    "    def __init__(self, config, verbose):\n",
    "        super(DensePolyNN, self).__init__()\n",
    "\n",
    "        img_dim = config['shape'][2]\n",
    "        channels = config['shape'][1]\n",
    "        linear_layers = [\n",
    "            LazyLinear(max(math.floor(channels * img_dim ** 2 / 2 ** (depth + 1)), channels)) for depth in\n",
    "            range(config['factors'].get('nr_linear_layers', 3) - 1)\n",
    "        ]\n",
    "\n",
    "        self.layers = Sequential(\n",
    "            Flatten(start_dim=1),\n",
    "            *linear_layers,\n",
    "            LazyLinear(config['labels'])\n",
    "        )\n",
    "        if verbose:\n",
    "            logging.info(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseLinearNN(Module):\n",
    "    def __init__(self, config, verbose):\n",
    "        super(DenseLinearNN, self).__init__()\n",
    "\n",
    "        img_dim = config['shape'][2]\n",
    "        channels = config['shape'][1]\n",
    "        linear_layers = [\n",
    "            LazyLinear(max(math.floor(\n",
    "                channels * img_dim ** 2 - (depth + 1) * (\n",
    "                        channels * img_dim ** 2 / config['factors'].get('nr_linear_layers', 3))),\n",
    "                channels)) for depth in range(config['factors'].get('nr_linear_layers', 3) - 1)]\n",
    "\n",
    "        self.layers = Sequential(\n",
    "            Flatten(start_dim=1),\n",
    "            *linear_layers,\n",
    "            LazyLinear(config['labels'])\n",
    "        )\n",
    "        if verbose:\n",
    "            logging.info(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model operations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [],
   "source": [
    "def create_model(net, config, verbose):\n",
    "    model = net(config=config, verbose=verbose)\n",
    "    optimizer = Adam(model.parameters(),\n",
    "                     lr=config['factors'].get('learning_rate', 1e-3),\n",
    "                     betas=(config['factors'].get('beta1', 0.9), config.get('beta2', 0.999)),\n",
    "                     eps=config['factors'].get('epsilon', 1e-8),\n",
    "                     weight_decay=config['factors'].get('weight_decay', 0)\n",
    "                     )\n",
    "    criterion = CrossEntropyLoss()\n",
    "    if torch.cuda.is_available():\n",
    "        logging.info('Using GPU')\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "\n",
    "def train(net, config, batches, tag, verbose=False):\n",
    "    logging.info(f'Using {config[\"factors\"]}')\n",
    "    logging.info(f'Constructing {tag}')\n",
    "    model, optimizer, criterion = create_model(net, config, verbose)\n",
    "    train_losses = []\n",
    "    logging.info('Training the model')\n",
    "    for _ in tqdm(range(config['epochs'])):\n",
    "        for batch_id, batch in batches:\n",
    "            train_x = batch[0]\n",
    "            train_y = batch[1]\n",
    "            model.train()\n",
    "            train_x, train_y = Variable(train_x), Variable(train_y)\n",
    "            if torch.cuda.is_available():\n",
    "                train_x = train_x.cuda()\n",
    "                train_y = train_y.cuda()\n",
    "\n",
    "            # clearing the Gradients of the model parameters\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # prediction for training set\n",
    "            output_train = model(train_x)\n",
    "\n",
    "            # computing the training loss\n",
    "            loss_train = criterion(output_train, train_y)\n",
    "            train_losses.append(loss_train.item())\n",
    "\n",
    "            # computing the updated weights of all the model parameters\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "    if verbose:\n",
    "        plt.plot(train_losses, label='Training loss')\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return model, tag, config['factors']\n",
    "\n",
    "\n",
    "def test(models, test_x, test_y):\n",
    "    logging.info('Generating predictions and calculating accuracy')\n",
    "    accuracies = []\n",
    "    for model, tag, factors in models:\n",
    "        with torch.no_grad():\n",
    "            output = model(test_x.cuda())\n",
    "\n",
    "        softmax = torch.exp(output).cpu()\n",
    "        prob = list(softmax.numpy())\n",
    "        predictions = np.argmax(prob, axis=1)\n",
    "        accuracy = accuracy_score(test_y, predictions)\n",
    "        accuracies.append((tag, accuracy, factors))\n",
    "        logging.info(f'{tag}: {accuracy=}')\n",
    "    return accuracies\n",
    "\n",
    "\n",
    "def predict(model, test_x, predictions):\n",
    "    logging.info('Generating predictions')\n",
    "    with torch.no_grad():\n",
    "        output = model(test_x.cuda())\n",
    "\n",
    "    softmax = torch.exp(output).cpu()\n",
    "    prob = list(softmax.numpy())\n",
    "    predictions['label'] = np.argmax(prob, axis=1)\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training strategies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [],
   "source": [
    "def random_strategy(factors_bounds, random_function, tag):\n",
    "    \"\"\"\n",
    "    Returns a configuration of (hyper)parameter values according to a random strategy\n",
    "    :param factors_bounds: The lower and upper bound for all the values\n",
    "    :param random_function: The method for generating random samples\n",
    "    :param tag: NOT USED, PASS NONE VALUE\n",
    "    :return: (hyper)parameter configuration\n",
    "    \"\"\"\n",
    "    logging.info('Applying the random strategy...')\n",
    "\n",
    "    factors = []\n",
    "    lowers = []\n",
    "    uppers = []\n",
    "    is_ints = []\n",
    "    for factor, (low, up, is_int) in factors_bounds.items():\n",
    "        factors.append(factor)\n",
    "        lowers.append(low)\n",
    "        uppers.append(up)\n",
    "        is_ints.append(is_int)\n",
    "\n",
    "    return random_function(factors, 1, lowers, uppers, is_ints)[0]\n",
    "\n",
    "\n",
    "def grid_search(factors_bounds, random_function, tag):\n",
    "    \"\"\"\n",
    "    Returns a configuration of (hyper)parameter values according to a grid search strategy\n",
    "    :param factors_bounds: The lower and upper bound for all the values\n",
    "    :param random_function: NOT USED, PASS NONE VALUE\n",
    "    :param tag: The tag of the neural network\n",
    "    :return: (hyper)parameter configuration\n",
    "    \"\"\"\n",
    "    logging.info('Applying a grid search strategy...')\n",
    "    if tag not in grid_search_space:\n",
    "        all_values = []\n",
    "        size = 1\n",
    "        real_value_split = 5\n",
    "        for factor, (lower, upper, is_int) in factors_bounds.items():\n",
    "            if is_int:\n",
    "                all_values.append([lower + i for i in range(upper-lower)])\n",
    "                size *= (upper-lower)\n",
    "            else:\n",
    "                all_values.append([lower + i * (upper-lower) / real_value_split for i in range(real_value_split)])\n",
    "                size *= real_value_split\n",
    "        search_space = list(product(*all_values))\n",
    "        random_indices = random.sample(range(size), config['experiments'])\n",
    "        grid_search_space[tag] = (random_indices, search_space)\n",
    "\n",
    "    random_indices, search_space = grid_search_space[tag]\n",
    "    factors = list(factors_bounds.keys())\n",
    "    random_idx = random_indices.pop()\n",
    "    return {factors[i]: search_space[random_idx][i] for i in range(len(factors))}\n",
    "\n",
    "def bayesian_pi(factor_bounds, random_function, tag):\n",
    "    \"\"\"\n",
    "    Returns a configuration of (hyper)parameter values according to a Bayesian strategy with probability of improvement\n",
    "    :param factors_bounds: The lower and upper bound for all the values\n",
    "    :param random_function: The method for generating random samples\n",
    "    :param tag: The tag of the neural network\n",
    "    :return: (hyper)parameter configuration\n",
    "    \"\"\"\n",
    "    logging.info('Applying the bayesian strategy with probability of improvement...')\n",
    "\n",
    "    factors = []\n",
    "    lowers = []\n",
    "    uppers = []\n",
    "    is_ints = []\n",
    "    for factor, (low, up, is_int) in factor_bounds.items():\n",
    "        factors.append(factor)\n",
    "        lowers.append(low)\n",
    "        uppers.append(up)\n",
    "        is_ints.append(is_int)\n",
    "\n",
    "    # Return a random sample and initialize the regression model if it does not exist\n",
    "    if tag not in regression_models:\n",
    "        random_factors = random_function(factors, 1, lowers, uppers, is_ints)[0]\n",
    "        regression_models[tag] = {\n",
    "            \"model\": GaussianProcessRegressor(),\n",
    "            \"X\": [],\n",
    "            \"y\": []\n",
    "        }\n",
    "        return random_factors\n",
    "\n",
    "    # Generate 10000 random samples and return the one with the highest probability of improvement, update the regression model\n",
    "    regression_models[tag]['model'].fit(regression_models[tag]['X'], regression_models[tag]['y'])\n",
    "    random_samples = random_function(factors, 10000, lowers, uppers, is_ints)\n",
    "    candidate_sample = best_probability_of_improvement(factors, random_samples, regression_models[tag]['model'],\n",
    "                                                       max(regression_models[tag]['y']))\n",
    "    return candidate_sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [],
   "source": [
    "def get_quasi_random_samples(factors, length, lower, upper, is_int, sampler=qmc.Halton):\n",
    "    sampler = sampler(len(factors))\n",
    "    sample = sampler.random(length)\n",
    "    return qmc.scale(sample, lower, upper)\n",
    "\n",
    "\n",
    "def get_random_samples(factors, length, lower, upper, is_int):\n",
    "    return [\n",
    "        {factors[i]: round(random.uniform(lower[i], upper[i])) if is_int[i] else random.uniform(lower[i], upper[i]) for\n",
    "         i in range(len(factors))} for _ in range(length)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Acquisition functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [],
   "source": [
    "def best_probability_of_improvement(factors, samples, model, y_best, maximize=True):\n",
    "    best = -1\n",
    "    x_next = None\n",
    "    for x in samples:\n",
    "        x = list(x.values())\n",
    "        with catch_warnings():\n",
    "            # ignore generated warnings\n",
    "            simplefilter(\"ignore\")\n",
    "            mu_, std_ = model.predict([x], return_std=True)\n",
    "            pi = norm.cdf((mu_ - y_best) / (std_ + 1e-9)) if maximize else norm.cdf((y_best - mu_) / (std_ + 1e-9))\n",
    "            if pi > best:\n",
    "                best = pi\n",
    "                x_next = x\n",
    "    return {factors[i]: x_next[i] for i in range(len(factors))}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [],
   "source": [
    "# Configurable parameters:\n",
    "# nr_linear_layers: default 3\n",
    "# nr_conv_layers: default 2\n",
    "# learning_rate: default 1e-3\n",
    "# beta1: default 0.9\n",
    "# beta2: default 0.999\n",
    "# epsilon: default 1e-8\n",
    "# weight_decay: default 0\n",
    "config = {\n",
    "    'experiments': 5,\n",
    "    'repetitions': 2,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 512,\n",
    "    'dataset': datasets.FashionMNIST,\n",
    "    'random_function': get_random_samples,\n",
    "    'optimization_strategy': grid_search,\n",
    "    'factor_bounds': {\n",
    "        'nr_linear_layers': (1, 8, True),\n",
    "        'learning_rate': (1e-5, 5e-3, False),\n",
    "        'beta1': (0.4, 1, False),\n",
    "        'beta2': (0.9, 1, False),\n",
    "        'epsilon': (1e-9, 1e-7, False),\n",
    "        'weight_decay': (0, 0.1, False)\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:25:53 INFO: Loading data...\n"
     ]
    }
   ],
   "source": [
    "logging.info('Loading data...')\n",
    "training_data = config['dataset'](\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "train_dataloader = DataLoader(training_data, batch_size=config['batch_size'], shuffle=True)\n",
    "batches = [(batch_id, batch) for batch_id, batch in enumerate(train_dataloader)]\n",
    "logging.info('Training data loaded')\n",
    "\n",
    "test_data = config['dataset'](\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "test_x, test_y = next(iter(test_dataloader))\n",
    "logging.info('Test data loaded')\n",
    "config['shape'] = batches[0][1][0].shape\n",
    "config['labels'] = len(training_data.classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = 43\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "results = defaultdict(list)\n",
    "regression_models = {}\n",
    "grid_search_space = {}\n",
    "####################\n",
    "nets_to_train = [DensePolyNN]\n",
    "####################\n",
    "for experiment in range(config['experiments']):\n",
    "    logging.info(f'\\n\\n#### RUNNING EXPERIMENT {experiment + 1}/{config[\"experiments\"]} ####\\n')\n",
    "    models = []\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for net in nets_to_train:\n",
    "            config['factors'] = config['optimization_strategy'](config['factor_bounds'], config['random_function'], net.__name__)\n",
    "            for _ in range(config['repetitions']):\n",
    "                models.append(train(DensePolyNN, config, batches, net.__name__))\n",
    "\n",
    "    res = test(models, test_x, test_y)\n",
    "    for tag, acc, used_config in res:\n",
    "        if tag in regression_models:\n",
    "            regression_models[tag]['X'].append(list(used_config.values()))\n",
    "            regression_models[tag]['y'].append(acc)\n",
    "        results[tag].append((experiment, acc, used_config))\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_name = f'exp_results_{datetime.today().strftime(\"%d%m%Y_%H%M%S\")}.csv'\n",
    "file = open(f'results/{file_name}', 'a')\n",
    "for tag in results:\n",
    "    print(f'{tag}:')\n",
    "    file.write(f'{tag}:\\n')\n",
    "    print(f'experiment,accuracy,{\"\".join(key + \",\" for key in results[tag][0][2])[:-1]}')\n",
    "    file.write(f'experiment,accuracy,{\"\".join(key + \",\" for key in results[tag][0][2])[:-1]}\\n')\n",
    "    [print(f'{experiment},{acc},{\"\".join(str(val) + \",\" for val in used_config.values())[:-1]}') for experiment, acc, used_config in results[tag]]\n",
    "    file.writelines([f'{experiment},{acc},{\"\".join(str(val) + \",\" for val in used_config.values())[:-1]}\\n' for experiment, acc, used_config in results[tag]])\n",
    "file.close()\n",
    "logging.info(f'Results logged to: results/{file_name}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}