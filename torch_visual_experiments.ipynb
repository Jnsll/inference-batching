{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from importlib import reload\n",
    "from warnings import catch_warnings, simplefilter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import qmc, norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import *\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s', level=logging.INFO, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network Classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "class SimpleCNN(Module):\n",
    "    def __init__(self, config, verbose):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        img_dim = config['shape'][2]\n",
    "        channels = config['shape'][1]\n",
    "        conv_layers = np.array([\n",
    "            [LazyConv2d(channels * 3 ** (depth + 1), kernel_size=3, stride=1, padding=1),\n",
    "             LazyBatchNorm2d(),\n",
    "             ReLU(inplace=True),\n",
    "             MaxPool2d(kernel_size=2, stride=2)]\n",
    "            for depth in range(config['factors']['nr_conv_layers'])\n",
    "        ]).flatten().tolist()\n",
    "        linear_layers = [\n",
    "            LazyLinear(max(math.floor(channels * img_dim ** 2 / 2 ** (depth + 1)), channels)) for depth in\n",
    "            range(config['factors']['nr_linear_layers'] - 1)\n",
    "        ]\n",
    "        self.layers = Sequential(\n",
    "            *conv_layers,\n",
    "            Flatten(start_dim=1),\n",
    "            *linear_layers,\n",
    "            LazyLinear(config['labels'])\n",
    "        )\n",
    "        if verbose:\n",
    "            logging.info(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DensePolyNN(Module):\n",
    "    def __init__(self, config, verbose):\n",
    "        super(DensePolyNN, self).__init__()\n",
    "\n",
    "        img_dim = config['shape'][2]\n",
    "        channels = config['shape'][1]\n",
    "        linear_layers = [\n",
    "            LazyLinear(max(math.floor(channels * img_dim ** 2 / 2 ** (depth + 1)), channels)) for depth in\n",
    "            range(config['factors']['nr_linear_layers'] - 1)\n",
    "        ]\n",
    "\n",
    "        self.layers = Sequential(\n",
    "            Flatten(start_dim=1),\n",
    "            *linear_layers,\n",
    "            LazyLinear(config['labels'])\n",
    "        )\n",
    "        if verbose:\n",
    "            logging.info(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseLinearNN(Module):\n",
    "    def __init__(self, config, verbose):\n",
    "        super(DenseLinearNN, self).__init__()\n",
    "\n",
    "        img_dim = config['shape'][2]\n",
    "        channels = config['shape'][1]\n",
    "        linear_layers = [\n",
    "            LazyLinear(max(math.floor(\n",
    "                channels * img_dim ** 2 - (depth + 1) * (\n",
    "                        channels * img_dim ** 2 / config['factors']['nr_linear_layers'])),\n",
    "                channels)) for depth in range(config['factors']['nr_linear_layers'] - 1)]\n",
    "\n",
    "        self.layers = Sequential(\n",
    "            Flatten(start_dim=1),\n",
    "            *linear_layers,\n",
    "            LazyLinear(config['labels'])\n",
    "        )\n",
    "        if verbose:\n",
    "            logging.info(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model operations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def create_model(net, config, verbose):\n",
    "    model = net(config=config, verbose=verbose)\n",
    "    optimizer = Adam(model.parameters(), lr=0.005)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    if torch.cuda.is_available():\n",
    "        logging.info('Using GPU')\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "\n",
    "def train(net, config, batches, tag, verbose=False):\n",
    "    logging.info(f'Constructing {tag}')\n",
    "    model, optimizer, criterion = create_model(net, config, verbose)\n",
    "    train_losses = []\n",
    "    logging.info('Training the model')\n",
    "    for _ in tqdm(range(config['epochs'])):\n",
    "        for batch_id, batch in batches:\n",
    "            train_x = batch[0]\n",
    "            train_y = batch[1]\n",
    "            model.train()\n",
    "            x_train, y_train = Variable(train_x), Variable(train_y)\n",
    "            if torch.cuda.is_available():\n",
    "                x_train = x_train.cuda()\n",
    "                y_train = y_train.cuda()\n",
    "\n",
    "            # clearing the Gradients of the model parameters\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # prediction for training set\n",
    "            output_train = model(x_train)\n",
    "\n",
    "            # computing the training loss\n",
    "            loss_train = criterion(output_train, y_train)\n",
    "            train_losses.append(loss_train.item())\n",
    "\n",
    "            # computing the updated weights of all the model parameters\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "    if verbose:\n",
    "        plt.plot(train_losses, label='Training loss')\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return model\n",
    "\n",
    "\n",
    "def test(models, test_x, test_y):\n",
    "    logging.info('Generating predictions and calculating accuracy')\n",
    "    accuracies = []\n",
    "    for model, tag in models:\n",
    "        with torch.no_grad():\n",
    "            output = model(test_x.cuda())\n",
    "\n",
    "        softmax = torch.exp(output).cpu()\n",
    "        prob = list(softmax.numpy())\n",
    "        predictions = np.argmax(prob, axis=1)\n",
    "        accuracy = accuracy_score(test_y, predictions)\n",
    "        accuracies.append((tag, accuracy))\n",
    "        logging.info(f'{tag}: {accuracy=}')\n",
    "    return accuracies\n",
    "\n",
    "\n",
    "def predict(model, test_x, predictions):\n",
    "    logging.info('Generating predictions')\n",
    "    with torch.no_grad():\n",
    "        output = model(test_x.cuda())\n",
    "\n",
    "    softmax = torch.exp(output).cpu()\n",
    "    prob = list(softmax.numpy())\n",
    "    predictions['label'] = np.argmax(prob, axis=1)\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training strategies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "def random_strategy(factors_bounds, random_function):\n",
    "    logging.info('Applying a random strategy...')\n",
    "\n",
    "    factors = []\n",
    "    lowers = []\n",
    "    uppers = []\n",
    "    is_ints = []\n",
    "    for factor, (low, up, is_int) in factors_bounds.items():\n",
    "        factors.append(factor)\n",
    "        lowers.append(low)\n",
    "        uppers.append(up)\n",
    "        is_ints.append(is_int)\n",
    "\n",
    "    return random_function(factors, 1, lowers, uppers, is_ints)[0]\n",
    "\n",
    "# def run_grid_experiments(args):\n",
    "#     logging.info('Applying a grid search strategy...')\n",
    "#     return args\n",
    "#\n",
    "#\n",
    "# def run_bayesian_pi_experiments(args):\n",
    "#     logging.info('Applying a bayesian strategy...')\n",
    "#     random_function = random_functions[args.exp_rand]\n",
    "#     acquisition_function = acquisition_functions[args.exp_acq]\n",
    "#     results = ['epoch,accuracy,eval_loss,config']\n",
    "#     dim, epochs, lower, upper, factors, constants = __parse_config(args)\n",
    "#     X = random_function(dim, 1, lower, upper)\n",
    "#     y = []\n",
    "#     model = GaussianProcessRegressor()\n",
    "#     for i in range(epochs):\n",
    "#         if i > 0:\n",
    "#             model.fit(X, y)\n",
    "#             candidate_samples = random_function(dim, 1000, lower, upper)\n",
    "#             X.append(acquisition_function(candidate_samples, model, max(y)))\n",
    "#         random.seed(INITIAL_SEED + i)\n",
    "#         # TODO: Make compatible for non-numeric values!\n",
    "#         factor_values = {key: value for key, value in zip(factors, X[i])}\n",
    "#         set_variables(args, {**factor_values, **constants}, i)\n",
    "#         acc, loss = __run_deberta(args)\n",
    "#         results.append(f'{i}\\t{acc}\\t{loss}\\t{factor_values}')\n",
    "#         y.append(acc)\n",
    "#     print(f'Best accuracy found for bayesian: {max(y)}')\n",
    "#     return results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "def get_quasi_random_samples(factors, length, lower, upper, is_int, sampler=qmc.Halton):\n",
    "    sampler = sampler(len(factors))\n",
    "    sample = sampler.random(length)\n",
    "    return qmc.scale(sample, lower, upper)\n",
    "\n",
    "\n",
    "def get_random_samples(factors, length, lower, upper, is_int):\n",
    "    return [{factors[i]: round(random.uniform(lower[i], upper[i])) if is_int[i] else random.uniform(lower[i], upper[i]) for i in range(len(factors))} for _ in range(length)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Acquisition functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def best_probability_of_improvement(samples, model, y_best, maximize=True):\n",
    "    best = -1\n",
    "    x_next = None\n",
    "    for x in samples:\n",
    "        with catch_warnings():\n",
    "            # ignore generated warnings\n",
    "            simplefilter(\"ignore\")\n",
    "            mu_, std_ = model.predict([x], return_std=True)\n",
    "            pi = norm.cdf((mu_ - y_best) / (std_ + 1e-9)) if maximize else norm.cdf((y_best - mu_) / (std_ + 1e-9))\n",
    "            if pi > best:\n",
    "                best = pi\n",
    "                x_next = x\n",
    "    return x_next"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "config = {\n",
    "    'experiments': 10,\n",
    "    'epochs': 25,\n",
    "    'batch_size': 512,\n",
    "    'dataset': datasets.FashionMNIST,\n",
    "    'random_function': get_random_samples,\n",
    "    'optimization_strategy': random_strategy,\n",
    "    'factor_bounds': {\n",
    "        'nr_linear_layers': (1, 20, True),\n",
    "        'nr_conv_layers': (1, 4, True),\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:07:44 INFO: Loading data...\n",
      "04:07:49 INFO: Training data loaded\n",
      "04:07:50 INFO: Test data loaded\n"
     ]
    }
   ],
   "source": [
    "logging.info('Loading data...')\n",
    "training_data = config['dataset'](\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "train_dataloader = DataLoader(training_data, batch_size=config['batch_size'], shuffle=True)\n",
    "batches = [(batch_id, batch) for batch_id, batch in enumerate(train_dataloader)]\n",
    "logging.info('Training data loaded')\n",
    "\n",
    "test_data = config['dataset'](\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "test_x, test_y = next(iter(test_dataloader))\n",
    "logging.info('Test data loaded')\n",
    "config['shape'] = batches[0][1][0].shape\n",
    "config['labels'] = len(training_data.classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:07:50 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 1/10 ####\n",
      "\n",
      "04:07:50 INFO: Applying a random strategy...\n",
      "04:07:50 INFO: Current config: {'nr_linear_layers': 8, 'nr_conv_layers': 4}\n",
      "04:07:50 INFO: Constructing SimpleCNN\n",
      "04:07:50 INFO: Using GPU\n",
      "04:07:50 INFO: Training the model\n",
      "100%|██████████| 25/25 [00:25<00:00,  1.01s/it]\n",
      "04:08:15 INFO: Generating predictions and calculating accuracy\n",
      "04:08:15 INFO: SimpleCNN: accuracy=0.874\n",
      "04:08:15 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 2/10 ####\n",
      "\n",
      "04:08:15 INFO: Applying a random strategy...\n",
      "04:08:15 INFO: Current config: {'nr_linear_layers': 10, 'nr_conv_layers': 1}\n",
      "04:08:15 INFO: Constructing SimpleCNN\n",
      "04:08:15 INFO: Using GPU\n",
      "04:08:15 INFO: Training the model\n",
      "100%|██████████| 25/25 [00:14<00:00,  1.74it/s]\n",
      "04:08:30 INFO: Generating predictions and calculating accuracy\n",
      "04:08:30 INFO: SimpleCNN: accuracy=0.5552\n",
      "04:08:30 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 3/10 ####\n",
      "\n",
      "04:08:30 INFO: Applying a random strategy...\n",
      "04:08:30 INFO: Current config: {'nr_linear_layers': 16, 'nr_conv_layers': 2}\n",
      "04:08:30 INFO: Constructing SimpleCNN\n",
      "04:08:30 INFO: Using GPU\n",
      "04:08:30 INFO: Training the model\n",
      "100%|██████████| 25/25 [00:20<00:00,  1.22it/s]\n",
      "04:08:50 INFO: Generating predictions and calculating accuracy\n",
      "04:08:50 INFO: SimpleCNN: accuracy=0.5094\n",
      "04:08:50 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 4/10 ####\n",
      "\n",
      "04:08:50 INFO: Applying a random strategy...\n",
      "04:08:50 INFO: Current config: {'nr_linear_layers': 15, 'nr_conv_layers': 2}\n",
      "04:08:50 INFO: Constructing SimpleCNN\n",
      "04:08:50 INFO: Using GPU\n",
      "04:08:50 INFO: Training the model\n",
      "100%|██████████| 25/25 [00:20<00:00,  1.24it/s]\n",
      "04:09:10 INFO: Generating predictions and calculating accuracy\n",
      "04:09:10 INFO: SimpleCNN: accuracy=0.6333\n",
      "04:09:11 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 5/10 ####\n",
      "\n",
      "04:09:11 INFO: Applying a random strategy...\n",
      "04:09:11 INFO: Current config: {'nr_linear_layers': 10, 'nr_conv_layers': 2}\n",
      "04:09:11 INFO: Constructing SimpleCNN\n",
      "04:09:11 INFO: Using GPU\n",
      "04:09:11 INFO: Training the model\n",
      "100%|██████████| 25/25 [00:18<00:00,  1.34it/s]\n",
      "04:09:29 INFO: Generating predictions and calculating accuracy\n",
      "04:09:29 INFO: SimpleCNN: accuracy=0.6503\n",
      "04:09:29 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 6/10 ####\n",
      "\n",
      "04:09:29 INFO: Applying a random strategy...\n",
      "04:09:29 INFO: Current config: {'nr_linear_layers': 19, 'nr_conv_layers': 2}\n",
      "04:09:29 INFO: Constructing SimpleCNN\n",
      "04:09:29 INFO: Using GPU\n",
      "04:09:29 INFO: Training the model\n",
      "100%|██████████| 25/25 [00:20<00:00,  1.22it/s]\n",
      "04:09:50 INFO: Generating predictions and calculating accuracy\n",
      "04:09:50 INFO: SimpleCNN: accuracy=0.6195\n",
      "04:09:50 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 7/10 ####\n",
      "\n",
      "04:09:50 INFO: Applying a random strategy...\n",
      "04:09:50 INFO: Current config: {'nr_linear_layers': 7, 'nr_conv_layers': 1}\n",
      "04:09:50 INFO: Constructing SimpleCNN\n",
      "04:09:50 INFO: Using GPU\n",
      "04:09:50 INFO: Training the model\n",
      "100%|██████████| 25/25 [00:13<00:00,  1.85it/s]\n",
      "04:10:03 INFO: Generating predictions and calculating accuracy\n",
      "04:10:03 INFO: SimpleCNN: accuracy=0.8717\n",
      "04:10:04 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 8/10 ####\n",
      "\n",
      "04:10:04 INFO: Applying a random strategy...\n",
      "04:10:04 INFO: Current config: {'nr_linear_layers': 12, 'nr_conv_layers': 3}\n",
      "04:10:04 INFO: Constructing SimpleCNN\n",
      "04:10:04 INFO: Using GPU\n",
      "04:10:04 INFO: Training the model\n",
      "100%|██████████| 25/25 [00:22<00:00,  1.10it/s]\n",
      "04:10:26 INFO: Generating predictions and calculating accuracy\n",
      "04:10:26 INFO: SimpleCNN: accuracy=0.4876\n",
      "04:10:26 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 9/10 ####\n",
      "\n",
      "04:10:26 INFO: Applying a random strategy...\n",
      "04:10:26 INFO: Current config: {'nr_linear_layers': 11, 'nr_conv_layers': 3}\n",
      "04:10:26 INFO: Constructing SimpleCNN\n",
      "04:10:26 INFO: Using GPU\n",
      "04:10:26 INFO: Training the model\n",
      "100%|██████████| 25/25 [00:22<00:00,  1.12it/s]\n",
      "04:10:49 INFO: Generating predictions and calculating accuracy\n",
      "04:10:49 INFO: SimpleCNN: accuracy=0.5122\n",
      "04:10:49 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 10/10 ####\n",
      "\n",
      "04:10:49 INFO: Applying a random strategy...\n",
      "04:10:49 INFO: Current config: {'nr_linear_layers': 19, 'nr_conv_layers': 3}\n",
      "04:10:49 INFO: Constructing SimpleCNN\n",
      "04:10:49 INFO: Using GPU\n",
      "04:10:49 INFO: Training the model\n",
      "100%|██████████| 25/25 [00:24<00:00,  1.03it/s]\n",
      "04:11:13 INFO: Generating predictions and calculating accuracy\n",
      "04:11:13 INFO: SimpleCNN: accuracy=0.6164\n"
     ]
    },
    {
     "data": {
      "text/plain": "defaultdict(list,\n            {'SimpleCNN': [0.874,\n              0.5552,\n              0.5094,\n              0.6333,\n              0.6503,\n              0.6195,\n              0.8717,\n              0.4876,\n              0.5122,\n              0.6164]})"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "accuracies = defaultdict(list)\n",
    "for experiment in range(config['experiments']):\n",
    "    logging.info(f'\\n\\n#### RUNNING EXPERIMENT {experiment + 1}/{config[\"experiments\"]} ####\\n')\n",
    "    config['factors'] = config['optimization_strategy'](config['factor_bounds'], config['random_function'])\n",
    "    logging.info(f'Current config: {config[\"factors\"]}')\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        models = [\n",
    "            # (train(DenseLinearNN, config, batches, 'DenseLinearNN'), 'DenseLinearNN'),\n",
    "            # (train(DensePolyNN, config, batches, 'DensePolyNN'), 'DensePolyNN'),\n",
    "            (train(SimpleCNN, config, batches, 'SimpleCNN'), 'SimpleCNN'),\n",
    "        ]\n",
    "    accs = test(models, test_x, test_y)\n",
    "    for tag, acc in accs:\n",
    "        accuracies[tag].append(acc)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "accuracies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}