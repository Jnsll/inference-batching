{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from importlib import reload\n",
    "from itertools import product\n",
    "from subprocess import Popen, PIPE\n",
    "from threading import Thread\n",
    "from warnings import catch_warnings, simplefilter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import qmc, norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import *\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s', level=logging.INFO, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network Classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class SimpleCNN(Module):\n",
    "    def __init__(self, config, verbose):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        img_dim = config['shape'][2]\n",
    "        channels = config['shape'][1]\n",
    "        conv_layers = np.array([\n",
    "            [LazyConv2d(channels * 3 ** (depth + 1), kernel_size=3, stride=1, padding=1),\n",
    "             LazyBatchNorm2d(),\n",
    "             ReLU(inplace=True),\n",
    "             MaxPool2d(kernel_size=2, stride=2)]\n",
    "            for depth in range(config['factors'].get('nr_conv_layers', 2))\n",
    "        ]).flatten().tolist()\n",
    "        linear_layers = [\n",
    "            LazyLinear(max(math.floor(channels * img_dim ** 2 / 2 ** (depth + 1)), channels)) for depth in\n",
    "            range(config['factors'].get('nr_linear_layers', 3) - 1)\n",
    "        ]\n",
    "        self.layers = Sequential(\n",
    "            *conv_layers,\n",
    "            Flatten(start_dim=1),\n",
    "            *linear_layers,\n",
    "            LazyLinear(config['labels'])\n",
    "        )\n",
    "        if verbose:\n",
    "            logging.info(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DensePolyNN(Module):\n",
    "    def __init__(self, config, verbose):\n",
    "        super(DensePolyNN, self).__init__()\n",
    "\n",
    "        img_dim = config['shape'][2]\n",
    "        channels = config['shape'][1]\n",
    "        linear_layers = [\n",
    "            LazyLinear(max(math.floor(channels * img_dim ** 2 / 2 ** (depth + 1)), channels)) for depth in\n",
    "            range(config['factors'].get('nr_linear_layers', 3) - 1)\n",
    "        ]\n",
    "\n",
    "        self.layers = Sequential(\n",
    "            Flatten(start_dim=1),\n",
    "            *linear_layers,\n",
    "            LazyLinear(config['labels'])\n",
    "        )\n",
    "        if verbose:\n",
    "            logging.info(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseLinearNN(Module):\n",
    "    def __init__(self, config, verbose):\n",
    "        super(DenseLinearNN, self).__init__()\n",
    "\n",
    "        img_dim = config['shape'][2]\n",
    "        channels = config['shape'][1]\n",
    "        linear_layers = [\n",
    "            LazyLinear(max(math.floor(\n",
    "                channels * img_dim ** 2 - (depth + 1) * (\n",
    "                        channels * img_dim ** 2 / config['factors'].get('nr_linear_layers', 3))),\n",
    "                channels)) for depth in range(config['factors'].get('nr_linear_layers', 3) - 1)]\n",
    "\n",
    "        self.layers = Sequential(\n",
    "            Flatten(start_dim=1),\n",
    "            *linear_layers,\n",
    "            LazyLinear(config['labels'])\n",
    "        )\n",
    "        if verbose:\n",
    "            logging.info(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get GPU values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class GPUMonitor(Thread):\n",
    "    def __init__(self, delay):\n",
    "        super(GPUMonitor, self).__init__()\n",
    "        self.delay = delay\n",
    "        self.power_readings = []\n",
    "        self.running = True\n",
    "        self.start()\n",
    "\n",
    "    def run(self):\n",
    "        while self.running:\n",
    "            try:\n",
    "                p = Popen('nvidia-smi --query-gpu=power.draw --format=csv,noheader,nounits'.split(' '), stdout=PIPE)\n",
    "                stdout, stderror = p.communicate()\n",
    "                self.power_readings.append(float(stdout.strip()))\n",
    "                p.terminate()\n",
    "            except:\n",
    "                logging.error('Something went wrong while retrieving GPU readings...')\n",
    "            time.sleep(self.delay)\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "\n",
    "    def reset_energy(self):\n",
    "        self.power_readings = []\n",
    "\n",
    "    def get_power_average(self):\n",
    "        return np.mean(self.power_readings)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model operations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def create_model(net, config, verbose):\n",
    "    model = net(config=config, verbose=verbose)\n",
    "    optimizer = Adam(model.parameters(),\n",
    "                     lr=config['factors'].get('learning_rate', 1e-3),\n",
    "                     betas=(config['factors'].get('beta1', 0.9), config.get('beta2', 0.999)),\n",
    "                     eps=config['factors'].get('epsilon', 1e-8),\n",
    "                     weight_decay=config['factors'].get('weight_decay', 0)\n",
    "                     )\n",
    "    criterion = CrossEntropyLoss()\n",
    "    if torch.cuda.is_available():\n",
    "        if verbose:\n",
    "            logging.info('Using GPU')\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "\n",
    "def train(net, config, batches, tag, verbose=False):\n",
    "    logging.info(f'Using {config[\"factors\"]}')\n",
    "    logging.info(f'Constructing {tag}')\n",
    "    model, optimizer, criterion = create_model(net, config, verbose)\n",
    "    train_losses = []\n",
    "    logging.info('Training the model:')\n",
    "    t_start = time.perf_counter()\n",
    "    gpu_monitor.reset_energy()\n",
    "    for _ in tqdm(range(config['epochs'])):\n",
    "        for batch_id, batch in batches:\n",
    "            train_x = batch[0]\n",
    "            train_y = batch[1]\n",
    "            model.train()\n",
    "            train_x, train_y = Variable(train_x), Variable(train_y)\n",
    "            if torch.cuda.is_available():\n",
    "                train_x = train_x.cuda()\n",
    "                train_y = train_y.cuda()\n",
    "\n",
    "            # clearing the Gradients of the model parameters\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # prediction for training set\n",
    "            output_train = model(train_x)\n",
    "\n",
    "            # computing the training loss\n",
    "            loss_train = criterion(output_train, train_y)\n",
    "            train_losses.append(loss_train.item())\n",
    "\n",
    "            # computing the updated weights of all the model parameters\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "    avg_power_ = gpu_monitor.get_power_average()\n",
    "    duration_ = time.perf_counter() - t_start\n",
    "    if verbose:\n",
    "        logging.info(f'{avg_power_=}, {duration_=}')\n",
    "        plt.plot(train_losses, label='Training loss')\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return model, tag, config['factors'], avg_power_, duration_\n",
    "\n",
    "\n",
    "def test(models, test_x, test_y):\n",
    "    logging.info('Generating predictions and calculating accuracy')\n",
    "    test_results = []\n",
    "    for model, tag, factors, avg_power, duration in models:\n",
    "        with torch.no_grad():\n",
    "            output = model(test_x.cuda())\n",
    "\n",
    "        softmax = torch.exp(output).cpu()\n",
    "        prob = list(softmax.numpy())\n",
    "        predictions = np.argmax(prob, axis=1)\n",
    "        accuracy = accuracy_score(test_y, predictions)\n",
    "        test_results.append((tag, accuracy, factors, avg_power, duration))\n",
    "        logging.info(f'{tag}: {accuracy=}')\n",
    "    return test_results\n",
    "\n",
    "\n",
    "def predict(model, test_x, predictions):\n",
    "    logging.info('Generating predictions')\n",
    "    with torch.no_grad():\n",
    "        output = model(test_x.cuda())\n",
    "\n",
    "    softmax = torch.exp(output).cpu()\n",
    "    prob = list(softmax.numpy())\n",
    "    predictions['label'] = np.argmax(prob, axis=1)\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training strategies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def random_strategy(factors_bounds, random_function, tag):\n",
    "    \"\"\"\n",
    "    Returns a configuration of (hyper)parameter values according to a random strategy\n",
    "    :param factors_bounds: The lower and upper bound for all the values\n",
    "    :param random_function: The method for generating random samples\n",
    "    :param tag: NOT USED, PASS NONE VALUE\n",
    "    :return: (hyper)parameter configuration\n",
    "    \"\"\"\n",
    "    logging.info('Applying the random strategy...')\n",
    "\n",
    "    factors = []\n",
    "    lowers = []\n",
    "    uppers = []\n",
    "    is_ints = []\n",
    "    for factor, (low, up, is_int) in factors_bounds.items():\n",
    "        factors.append(factor)\n",
    "        lowers.append(low)\n",
    "        uppers.append(up)\n",
    "        is_ints.append(is_int)\n",
    "\n",
    "    return random_function(factors, 1, lowers, uppers, is_ints)[0]\n",
    "\n",
    "\n",
    "def grid_search(factors_bounds, random_function, tag):\n",
    "    \"\"\"\n",
    "    Returns a configuration of (hyper)parameter values according to a grid search strategy\n",
    "    :param factors_bounds: The lower and upper bound for all the values\n",
    "    :param random_function: NOT USED, PASS NONE VALUE\n",
    "    :param tag: The tag of the neural network\n",
    "    :return: (hyper)parameter configuration\n",
    "    \"\"\"\n",
    "    logging.info('Applying a grid search strategy...')\n",
    "    if tag not in grid_search_space:\n",
    "        all_values = []\n",
    "        size = 1\n",
    "        real_value_split = 5\n",
    "        for factor, (lower, upper, is_int) in factors_bounds.items():\n",
    "            if is_int:\n",
    "                all_values.append([lower + i for i in range(upper - lower)])\n",
    "                size *= (upper - lower)\n",
    "            else:\n",
    "                all_values.append([lower + i * (upper - lower) / real_value_split for i in range(real_value_split)])\n",
    "                size *= real_value_split\n",
    "        search_space = list(product(*all_values))\n",
    "        random_indices = random.sample(range(size), config['experiments'])\n",
    "        grid_search_space[tag] = (random_indices, search_space)\n",
    "\n",
    "    random_indices, search_space = grid_search_space[tag]\n",
    "    factors = list(factors_bounds.keys())\n",
    "    random_idx = random_indices.pop()\n",
    "    return {factors[i]: search_space[random_idx][i] for i in range(len(factors))}\n",
    "\n",
    "\n",
    "def bayesian_pi(factor_bounds, random_function, tag):\n",
    "    \"\"\"\n",
    "    Returns a configuration of (hyper)parameter values according to a Bayesian strategy with probability of improvement\n",
    "    :param factors_bounds: The lower and upper bound for all the values\n",
    "    :param random_function: The method for generating random samples\n",
    "    :param tag: The tag of the neural network\n",
    "    :return: (hyper)parameter configuration\n",
    "    \"\"\"\n",
    "    logging.info('Applying the bayesian strategy with probability of improvement...')\n",
    "\n",
    "    factors = []\n",
    "    lowers = []\n",
    "    uppers = []\n",
    "    is_ints = []\n",
    "    for factor, (low, up, is_int) in factor_bounds.items():\n",
    "        factors.append(factor)\n",
    "        lowers.append(low)\n",
    "        uppers.append(up)\n",
    "        is_ints.append(is_int)\n",
    "\n",
    "    # Return a random sample and initialize the regression model if it does not exist\n",
    "    if tag not in regression_models:\n",
    "        random_factors = random_function(factors, 1, lowers, uppers, is_ints)[0]\n",
    "        regression_models[tag] = {\n",
    "            \"model\": GaussianProcessRegressor(),\n",
    "            \"X\": [],\n",
    "            \"y\": []\n",
    "        }\n",
    "        return random_factors\n",
    "\n",
    "    # Generate 10000 random samples and return the one with the highest probability of improvement, update the regression model\n",
    "    regression_models[tag]['model'].fit(regression_models[tag]['X'], regression_models[tag]['y'])\n",
    "    random_samples = random_function(factors, 10000, lowers, uppers, is_ints)\n",
    "    candidate_sample = best_probability_of_improvement(factors, random_samples, regression_models[tag]['model'],\n",
    "                                                       max(regression_models[tag]['y']))\n",
    "    return candidate_sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def quasi_random(factors, length, lower, upper, is_int, sampler=qmc.Halton):\n",
    "    sampler = sampler(len(factors))\n",
    "    sample = sampler.random(length)\n",
    "    random_samples = qmc.scale(sample, lower, upper)\n",
    "    return [{factors[i]: round(s[i]) if is_int[i] else s[i] for i in range(len(factors))} for s in random_samples]\n",
    "\n",
    "\n",
    "def uniform_random(factors, length, lower, upper, is_int):\n",
    "    return [\n",
    "        {factors[i]: round(random.uniform(lower[i], upper[i])) if is_int[i] else random.uniform(lower[i], upper[i]) for\n",
    "         i in range(len(factors))} for _ in range(length)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Acquisition functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def best_probability_of_improvement(factors, samples, model, y_best, maximize=True):\n",
    "    best = -1\n",
    "    x_next = None\n",
    "    for x in samples:\n",
    "        x = list(x.values())\n",
    "        with catch_warnings():\n",
    "            # ignore generated warnings\n",
    "            simplefilter(\"ignore\")\n",
    "            mu_, std_ = model.predict([x], return_std=True)\n",
    "            pi = norm.cdf((mu_ - y_best) / (std_ + 1e-9)) if maximize else norm.cdf((y_best - mu_) / (std_ + 1e-9))\n",
    "            if pi > best:\n",
    "                best = pi\n",
    "                x_next = x\n",
    "    return {factors[i]: x_next[i] for i in range(len(factors))}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Configurable parameters:\n",
    "# nr_linear_layers: default 3\n",
    "# nr_conv_layers: default 2\n",
    "# learning_rate: default 1e-3\n",
    "# beta1: default 0.9\n",
    "# beta2: default 0.999\n",
    "# epsilon: default 1e-8\n",
    "# weight_decay: default 0\n",
    "config = {\n",
    "    'experiments': 5,\n",
    "    'repetitions': 1,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 512,\n",
    "    'dataset': datasets.FashionMNIST,\n",
    "    'random_function': quasi_random,\n",
    "    'optimization_strategy': bayesian_pi,\n",
    "    'factor_bounds': {\n",
    "        'nr_linear_layers': (1, 8, True),\n",
    "        'learning_rate': (1e-5, 5e-3, False),\n",
    "        'beta1': (0.4, 1, False),\n",
    "        'beta2': (0.9, 1, False),\n",
    "        'epsilon': (1e-9, 1e-7, False),\n",
    "        'weight_decay': (0, 0.1, False)\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:19:26 INFO: Loading data...\n",
      "03:19:31 INFO: Training data loaded\n",
      "03:19:32 INFO: Test data loaded\n"
     ]
    }
   ],
   "source": [
    "logging.info('Loading data...')\n",
    "training_data = config['dataset'](\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "train_dataloader = DataLoader(training_data, batch_size=config['batch_size'], shuffle=True)\n",
    "batches = [(batch_id, batch) for batch_id, batch in enumerate(train_dataloader)]\n",
    "logging.info('Training data loaded')\n",
    "\n",
    "test_data = config['dataset'](\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "test_x, test_y = next(iter(test_dataloader))\n",
    "logging.info('Test data loaded')\n",
    "config['shape'] = batches[0][1][0].shape\n",
    "config['labels'] = len(training_data.classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:21:28 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 1/5 ####\n",
      "\n",
      "03:21:28 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "03:21:28 INFO: Using {'nr_linear_layers': 1, 'learning_rate': 0.004209464312628585, 'beta1': 0.7141144693328179, 'beta2': 0.9692403335171381, 'epsilon': 1.5842192434657596e-08, 'weight_decay': 0.020649188632317243}\n",
      "03:21:28 INFO: Constructing DensePolyNN\n",
      "03:21:28 INFO: Training the model:\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.33it/s]\n",
      "03:21:30 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "03:21:30 INFO: Using {'nr_linear_layers': 7, 'learning_rate': 0.002364574628503909, 'beta1': 0.7031980723821807, 'beta2': 0.9772449481299729, 'epsilon': 7.478416170828334e-08, 'weight_decay': 0.048183319524328645}\n",
      "03:21:30 INFO: Constructing SimpleCNN\n",
      "03:21:30 INFO: Training the model:\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.04it/s]\n",
      "03:21:35 INFO: Generating predictions and calculating accuracy\n",
      "03:21:35 INFO: DensePolyNN: accuracy=0.7984\n",
      "03:21:35 INFO: SimpleCNN: accuracy=0.0991\n",
      "03:21:35 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 2/5 ####\n",
      "\n",
      "03:21:35 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "03:21:38 INFO: Using {'nr_linear_layers': 1, 'learning_rate': 0.004352411593993898, 'beta1': 0.7170002497208019, 'beta2': 0.9734948786433103, 'epsilon': 1.702100001426253e-08, 'weight_decay': 0.02463118340287719}\n",
      "03:21:38 INFO: Constructing DensePolyNN\n",
      "03:21:38 INFO: Training the model:\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.08it/s]\n",
      "03:21:40 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "03:21:43 INFO: Using {'nr_linear_layers': 7, 'learning_rate': 0.0032702679616745845, 'beta1': 0.7008913012146343, 'beta2': 0.9703298402089348, 'epsilon': 6.38381972716814e-08, 'weight_decay': 0.05332895629239384}\n",
      "03:21:43 INFO: Constructing SimpleCNN\n",
      "03:21:43 INFO: Training the model:\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.19it/s]\n",
      "03:21:48 INFO: Generating predictions and calculating accuracy\n",
      "03:21:48 INFO: DensePolyNN: accuracy=0.7927\n",
      "03:21:48 INFO: SimpleCNN: accuracy=0.1\n",
      "03:21:48 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 3/5 ####\n",
      "\n",
      "03:21:48 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "03:21:51 INFO: Using {'nr_linear_layers': 1, 'learning_rate': 0.0015441282035786193, 'beta1': 0.6998102425176547, 'beta2': 0.9418235719113304, 'epsilon': 6.058995015082956e-09, 'weight_decay': 0.0053449788238321095}\n",
      "03:21:51 INFO: Constructing DensePolyNN\n",
      "03:21:51 INFO: Training the model:\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.30it/s]\n",
      "03:21:53 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "03:21:56 INFO: Using {'nr_linear_layers': 7, 'learning_rate': 0.0038390083519833436, 'beta1': 0.6864324811833724, 'beta2': 0.9098975858036853, 'epsilon': 6.020544283696626e-08, 'weight_decay': 0.09231964359829137}\n",
      "03:21:56 INFO: Constructing SimpleCNN\n",
      "03:21:56 INFO: Training the model:\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.10it/s]\n",
      "03:22:01 INFO: Generating predictions and calculating accuracy\n",
      "03:22:01 INFO: DensePolyNN: accuracy=0.826\n",
      "03:22:01 INFO: SimpleCNN: accuracy=0.1\n",
      "03:22:01 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 4/5 ####\n",
      "\n",
      "03:22:01 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "03:22:04 INFO: Using {'nr_linear_layers': 1, 'learning_rate': 0.0019305403740994, 'beta1': 0.6706524304230879, 'beta2': 0.9067077979119442, 'epsilon': 9.082539987981628e-09, 'weight_decay': 0.020669383351720097}\n",
      "03:22:04 INFO: Constructing DensePolyNN\n",
      "03:22:04 INFO: Training the model:\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.04it/s]\n",
      "03:22:06 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "03:22:09 INFO: Using {'nr_linear_layers': 7, 'learning_rate': 0.004652626848092898, 'beta1': 0.6890889442440602, 'beta2': 0.9322249362163825, 'epsilon': 3.149452978259691e-08, 'weight_decay': 0.07746507484178666}\n",
      "03:22:09 INFO: Constructing SimpleCNN\n",
      "03:22:09 INFO: Training the model:\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.27it/s]\n",
      "03:22:14 INFO: Generating predictions and calculating accuracy\n",
      "03:22:14 INFO: DensePolyNN: accuracy=0.8073\n",
      "03:22:14 INFO: SimpleCNN: accuracy=0.1\n",
      "03:22:14 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 5/5 ####\n",
      "\n",
      "03:22:14 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "03:22:17 INFO: Using {'nr_linear_layers': 1, 'learning_rate': 0.0018846035875471566, 'beta1': 0.7018373277067391, 'beta2': 0.9430680956169735, 'epsilon': 2.3003093424082845e-08, 'weight_decay': 0.003588750352586734}\n",
      "03:22:17 INFO: Constructing DensePolyNN\n",
      "03:22:17 INFO: Training the model:\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.16it/s]\n",
      "03:22:19 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "03:22:22 INFO: Using {'nr_linear_layers': 7, 'learning_rate': 0.0034725372017992496, 'beta1': 0.7202171424153279, 'beta2': 0.9928019627189818, 'epsilon': 3.1939725670901485e-09, 'weight_decay': 0.051027908705788154}\n",
      "03:22:22 INFO: Constructing SimpleCNN\n",
      "03:22:22 INFO: Training the model:\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.06it/s]\n",
      "03:22:27 INFO: Generating predictions and calculating accuracy\n",
      "03:22:27 INFO: DensePolyNN: accuracy=0.8282\n",
      "03:22:27 INFO: SimpleCNN: accuracy=0.1\n"
     ]
    }
   ],
   "source": [
    "seed = 45\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "results = defaultdict(list)\n",
    "regression_models = {}\n",
    "grid_search_space = {}\n",
    "gpu_monitor = GPUMonitor(0.1)\n",
    "####################\n",
    "nets_to_train = [DensePolyNN, SimpleCNN]\n",
    "####################\n",
    "for experiment in range(config['experiments']):\n",
    "    logging.info(f'\\n\\n#### RUNNING EXPERIMENT {experiment + 1}/{config[\"experiments\"]} ####\\n')\n",
    "    models = []\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for net in nets_to_train:\n",
    "            config['factors'] = config['optimization_strategy'](config['factor_bounds'], config['random_function'],\n",
    "                                                                net.__name__)\n",
    "            for _ in range(config['repetitions']):\n",
    "                t_start = time.perf_counter()\n",
    "                models.append(train(DensePolyNN, config, batches, net.__name__))\n",
    "\n",
    "    test_results = test(models, test_x, test_y)\n",
    "    for tag, acc, used_config, avg_power, duration in test_results:\n",
    "        if tag in regression_models:\n",
    "            regression_models[tag]['X'].append(list(used_config.values()))\n",
    "            regression_models[tag]['y'].append(acc)\n",
    "        results[tag].append((experiment, acc, used_config, avg_power, duration))\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "gpu_monitor.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write Results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:22:53 INFO: Results logged to: results/exp_results_27122021_152253.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DensePolyNN:\n",
      "experiment,accuracy,avg_power,duration,nr_linear_layers,learning_rate,beta1,beta2,epsilon,weight_decay\n",
      "0,0.7984,63.27333333333332,2.3113186379996478,1,0.004209464312628585,0.7141144693328179,0.9692403335171381,1.5842192434657596e-08,0.020649188632317243\n",
      "1,0.7927,63.654545454545456,1.9702858439995907,1,0.004352411593993898,0.7170002497208019,0.9734948786433103,1.702100001426253e-08,0.02463118340287719\n",
      "2,0.826,63.285833333333336,2.3259776719987713,1,0.0015441282035786193,0.6998102425176547,0.9418235719113304,6.058995015082956e-09,0.0053449788238321095\n",
      "3,0.8073,50.169000000000004,1.9853815659989777,1,0.0019305403740994,0.6706524304230879,0.9067077979119442,9.082539987981628e-09,0.020669383351720097\n",
      "4,0.8282,63.053999999999995,1.9415338979997614,1,0.0018846035875471566,0.7018373277067391,0.9430680956169735,2.3003093424082845e-08,0.003588750352586734\n",
      "SimpleCNN:\n",
      "experiment,accuracy,avg_power,duration,nr_linear_layers,learning_rate,beta1,beta2,epsilon,weight_decay\n",
      "0,0.0991,77.45076923076923,4.899274404000607,7,0.002364574628503909,0.7031980723821807,0.9772449481299729,7.478416170828334e-08,0.048183319524328645\n",
      "1,0.1,76.656,4.568292052999823,7,0.0032702679616745845,0.7008913012146343,0.9703298402089348,6.38381972716814e-08,0.05332895629239384\n",
      "2,0.1,76.59153846153846,4.7599997260003875,7,0.0038390083519833436,0.6864324811833724,0.9098975858036853,6.020544283696626e-08,0.09231964359829137\n",
      "3,0.1,63.04749999999999,4.440107700000226,7,0.004652626848092898,0.6890889442440602,0.9322249362163825,3.149452978259691e-08,0.07746507484178666\n",
      "4,0.1,75.65925925925926,4.848016472000381,7,0.0034725372017992496,0.7202171424153279,0.9928019627189818,3.1939725670901485e-09,0.051027908705788154\n"
     ]
    }
   ],
   "source": [
    "file_name = f'exp_results_{datetime.today().strftime(\"%d%m%Y_%H%M%S\")}.csv'\n",
    "file = open(f'results/{file_name}', 'a')\n",
    "for tag in results:\n",
    "    print(f'{tag}:')\n",
    "    file.write(f'{tag}:\\n')\n",
    "    print(f'experiment,accuracy,avg_power,duration,{\"\".join(key + \",\" for key in results[tag][0][2])[:-1]}')\n",
    "    file.write(f'experiment,accuracy,avg_power,duration,{\"\".join(key + \",\" for key in results[tag][0][2])[:-1]}\\n')\n",
    "    [print(f'{experiment},{acc},{avg_power},{duration},{\"\".join(str(val) + \",\" for val in used_config.values())[:-1]}')\n",
    "     for\n",
    "     experiment, acc, used_config, avg_power, duration in results[tag]]\n",
    "    file.writelines(\n",
    "        [f'{experiment},{acc},{avg_power},{duration},{\"\".join(str(val) + \",\" for val in used_config.values())[:-1]}\\n'\n",
    "         for\n",
    "         experiment, acc, used_config, avg_power, duration in results[tag]])\n",
    "file.close()\n",
    "logging.info(f'Results logged to: results/{file_name}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}