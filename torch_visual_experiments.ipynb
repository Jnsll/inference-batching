{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from importlib import reload\n",
    "from warnings import catch_warnings, simplefilter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import qmc, norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import *\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s', level=logging.INFO, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network Classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [],
   "source": [
    "class SimpleCNN(Module):\n",
    "    def __init__(self, config, verbose):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        img_dim = config['shape'][2]\n",
    "        channels = config['shape'][1]\n",
    "        conv_layers = np.array([\n",
    "            [LazyConv2d(channels * 3 ** (depth + 1), kernel_size=3, stride=1, padding=1),\n",
    "             LazyBatchNorm2d(),\n",
    "             ReLU(inplace=True),\n",
    "             MaxPool2d(kernel_size=2, stride=2)]\n",
    "            for depth in range(config['factors']['nr_conv_layers'])\n",
    "        ]).flatten().tolist()\n",
    "        linear_layers = [\n",
    "            LazyLinear(max(math.floor(channels * img_dim ** 2 / 2 ** (depth + 1)), channels)) for depth in\n",
    "            range(config['factors']['nr_linear_layers'] - 1)\n",
    "        ]\n",
    "        self.layers = Sequential(\n",
    "            *conv_layers,\n",
    "            Flatten(start_dim=1),\n",
    "            *linear_layers,\n",
    "            LazyLinear(config['labels'])\n",
    "        )\n",
    "        if verbose:\n",
    "            logging.info(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DensePolyNN(Module):\n",
    "    def __init__(self, config, verbose):\n",
    "        super(DensePolyNN, self).__init__()\n",
    "\n",
    "        img_dim = config['shape'][2]\n",
    "        channels = config['shape'][1]\n",
    "        linear_layers = [\n",
    "            LazyLinear(max(math.floor(channels * img_dim ** 2 / 2 ** (depth + 1)), channels)) for depth in\n",
    "            range(config['factors']['nr_linear_layers'] - 1)\n",
    "        ]\n",
    "\n",
    "        self.layers = Sequential(\n",
    "            Flatten(start_dim=1),\n",
    "            *linear_layers,\n",
    "            LazyLinear(config['labels'])\n",
    "        )\n",
    "        if verbose:\n",
    "            logging.info(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseLinearNN(Module):\n",
    "    def __init__(self, config, verbose):\n",
    "        super(DenseLinearNN, self).__init__()\n",
    "\n",
    "        img_dim = config['shape'][2]\n",
    "        channels = config['shape'][1]\n",
    "        linear_layers = [\n",
    "            LazyLinear(max(math.floor(\n",
    "                channels * img_dim ** 2 - (depth + 1) * (\n",
    "                        channels * img_dim ** 2 / config['factors']['nr_linear_layers'])),\n",
    "                channels)) for depth in range(config['factors']['nr_linear_layers'] - 1)]\n",
    "\n",
    "        self.layers = Sequential(\n",
    "            Flatten(start_dim=1),\n",
    "            *linear_layers,\n",
    "            LazyLinear(config['labels'])\n",
    "        )\n",
    "        if verbose:\n",
    "            logging.info(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model operations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [],
   "source": [
    "def create_model(net, config, verbose):\n",
    "    model = net(config=config, verbose=verbose)\n",
    "    optimizer = Adam(model.parameters(), lr=0.005)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    if torch.cuda.is_available():\n",
    "        logging.info('Using GPU')\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    return model, optimizer, criterion\n",
    "\n",
    "\n",
    "def train(net, config, batches, tag, verbose=False):\n",
    "    logging.info(f'Using {config[\"factors\"]}')\n",
    "    logging.info(f'Constructing {tag}')\n",
    "    model, optimizer, criterion = create_model(net, config, verbose)\n",
    "    train_losses = []\n",
    "    logging.info('Training the model')\n",
    "    for _ in tqdm(range(config['epochs'])):\n",
    "        for batch_id, batch in batches:\n",
    "            train_x = batch[0]\n",
    "            train_y = batch[1]\n",
    "            model.train()\n",
    "            x_train, y_train = Variable(train_x), Variable(train_y)\n",
    "            if torch.cuda.is_available():\n",
    "                x_train = x_train.cuda()\n",
    "                y_train = y_train.cuda()\n",
    "\n",
    "            # clearing the Gradients of the model parameters\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # prediction for training set\n",
    "            output_train = model(x_train)\n",
    "\n",
    "            # computing the training loss\n",
    "            loss_train = criterion(output_train, y_train)\n",
    "            train_losses.append(loss_train.item())\n",
    "\n",
    "            # computing the updated weights of all the model parameters\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "    if verbose:\n",
    "        plt.plot(train_losses, label='Training loss')\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return model, tag\n",
    "\n",
    "\n",
    "def test(models, test_x, test_y):\n",
    "    logging.info('Generating predictions and calculating accuracy')\n",
    "    accuracies = []\n",
    "    for model, tag in models:\n",
    "        with torch.no_grad():\n",
    "            output = model(test_x.cuda())\n",
    "\n",
    "        softmax = torch.exp(output).cpu()\n",
    "        prob = list(softmax.numpy())\n",
    "        predictions = np.argmax(prob, axis=1)\n",
    "        accuracy = accuracy_score(test_y, predictions)\n",
    "        accuracies.append((tag, accuracy))\n",
    "        logging.info(f'{tag}: {accuracy=}')\n",
    "    return accuracies\n",
    "\n",
    "\n",
    "def predict(model, test_x, predictions):\n",
    "    logging.info('Generating predictions')\n",
    "    with torch.no_grad():\n",
    "        output = model(test_x.cuda())\n",
    "\n",
    "    softmax = torch.exp(output).cpu()\n",
    "    prob = list(softmax.numpy())\n",
    "    predictions['label'] = np.argmax(prob, axis=1)\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training strategies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [],
   "source": [
    "def random_strategy(factors_bounds, random_function, tag):\n",
    "    logging.info('Applying the random strategy...')\n",
    "\n",
    "    factors = []\n",
    "    lowers = []\n",
    "    uppers = []\n",
    "    is_ints = []\n",
    "    for factor, (low, up, is_int) in factors_bounds.items():\n",
    "        factors.append(factor)\n",
    "        lowers.append(low)\n",
    "        uppers.append(up)\n",
    "        is_ints.append(is_int)\n",
    "\n",
    "    return random_function(factors, 1, lowers, uppers, is_ints)[0]\n",
    "\n",
    "# def run_grid_experiments(args):\n",
    "#     logging.info('Applying a grid search strategy...')\n",
    "#     return args\n",
    "\n",
    "def bayesian_pi(factor_bounds, random_function, tag):\n",
    "    logging.info('Applying the bayesian strategy with probability of improvement...')\n",
    "\n",
    "    factors = []\n",
    "    lowers = []\n",
    "    uppers = []\n",
    "    is_ints = []\n",
    "    for factor, (low, up, is_int) in factor_bounds.items():\n",
    "        factors.append(factor)\n",
    "        lowers.append(low)\n",
    "        uppers.append(up)\n",
    "        is_ints.append(is_int)\n",
    "\n",
    "    if tag not in regression_models:\n",
    "        random_factors = random_function(factors, 1, lowers, uppers, is_ints)[0]\n",
    "        regression_models[tag] = {\n",
    "            \"model\": GaussianProcessRegressor(),\n",
    "            \"X\": [list(random_factors.values())],\n",
    "            \"y\": []\n",
    "        }\n",
    "        return random_factors\n",
    "\n",
    "\n",
    "    regression_models[tag]['model'].fit(regression_models[tag]['X'], regression_models[tag]['y'])\n",
    "    random_samples = random_function(factors, 1000, lowers, uppers, is_ints)\n",
    "    candidate_sample = best_probability_of_improvement(factors, random_samples, regression_models[tag]['model'], max(regression_models[tag]['y']))\n",
    "    regression_models[tag]['X'].append(list(candidate_sample.values()))\n",
    "    return candidate_sample\n",
    "\n",
    "    # X.append(acquisition_function(candidate_samples, model, max(y)))\n",
    "    # random.seed(INITIAL_SEED + i)\n",
    "    # # TODO: Make compatible for non-numeric values!\n",
    "    # factor_values = {key: value for key, value in zip(factors, X[i])}\n",
    "    # set_variables(args, {**factor_values, **constants}, i)\n",
    "    # acc, loss = __run_deberta(args)\n",
    "    # results.append(f'{i}\\t{acc}\\t{loss}\\t{factor_values}')\n",
    "    # y.append(acc)\n",
    "    # print(f'Best accuracy found for bayesian: {max(y)}')\n",
    "    # return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [],
   "source": [
    "def get_quasi_random_samples(factors, length, lower, upper, is_int, sampler=qmc.Halton):\n",
    "    sampler = sampler(len(factors))\n",
    "    sample = sampler.random(length)\n",
    "    return qmc.scale(sample, lower, upper)\n",
    "\n",
    "\n",
    "def get_random_samples(factors, length, lower, upper, is_int):\n",
    "    return [{factors[i]: round(random.uniform(lower[i], upper[i])) if is_int[i] else random.uniform(lower[i], upper[i]) for i in range(len(factors))} for _ in range(length)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Acquisition functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "outputs": [],
   "source": [
    "def best_probability_of_improvement(factors, samples, model, y_best, maximize=True):\n",
    "    best = -1\n",
    "    x_next = None\n",
    "    for x in samples:\n",
    "        x = list(x.values())\n",
    "        with catch_warnings():\n",
    "            # ignore generated warnings\n",
    "            simplefilter(\"ignore\")\n",
    "            mu_, std_ = model.predict([x], return_std=True)\n",
    "            pi = norm.cdf((mu_ - y_best) / (std_ + 1e-9)) if maximize else norm.cdf((y_best - mu_) / (std_ + 1e-9))\n",
    "            if pi > best:\n",
    "                best = pi\n",
    "                x_next = x\n",
    "    return {factors[i]: x_next[i] for i in range(len(factors))}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "outputs": [],
   "source": [
    "config = {\n",
    "    'experiments': 20,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 512,\n",
    "    'dataset': datasets.FashionMNIST,\n",
    "    'random_function': get_random_samples,\n",
    "    'optimization_strategy': bayesian_pi,\n",
    "    'factor_bounds': {\n",
    "        'nr_linear_layers': (1, 20, True),\n",
    "        'nr_conv_layers': (1, 4, True),\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:43:56 INFO: Loading data...\n",
      "05:44:01 INFO: Training data loaded\n",
      "05:44:01 INFO: Test data loaded\n"
     ]
    }
   ],
   "source": [
    "logging.info('Loading data...')\n",
    "training_data = config['dataset'](\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "train_dataloader = DataLoader(training_data, batch_size=config['batch_size'], shuffle=True)\n",
    "batches = [(batch_id, batch) for batch_id, batch in enumerate(train_dataloader)]\n",
    "logging.info('Training data loaded')\n",
    "\n",
    "test_data = config['dataset'](\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "test_x, test_y = next(iter(test_dataloader))\n",
    "logging.info('Test data loaded')\n",
    "config['shape'] = batches[0][1][0].shape\n",
    "config['labels'] = len(training_data.classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:44:01 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 1/20 ####\n",
      "\n",
      "05:44:01 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:44:01 INFO: Using {'nr_linear_layers': 13, 'nr_conv_layers': 1}\n",
      "05:44:01 INFO: Constructing DensePolyNN\n",
      "05:44:01 INFO: Using GPU\n",
      "05:44:01 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.37it/s]\n",
      "05:44:06 INFO: Generating predictions and calculating accuracy\n",
      "05:44:06 INFO: DensePolyNN: accuracy=0.4735\n",
      "05:44:06 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 2/20 ####\n",
      "\n",
      "05:44:06 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:44:06 INFO: Using {'nr_linear_layers': 13, 'nr_conv_layers': 1}\n",
      "05:44:06 INFO: Constructing DensePolyNN\n",
      "05:44:06 INFO: Using GPU\n",
      "05:44:06 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.69it/s]\n",
      "05:44:10 INFO: Generating predictions and calculating accuracy\n",
      "05:44:10 INFO: DensePolyNN: accuracy=0.5165\n",
      "05:44:10 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 3/20 ####\n",
      "\n",
      "05:44:10 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:44:10 INFO: Using {'nr_linear_layers': 13, 'nr_conv_layers': 2}\n",
      "05:44:10 INFO: Constructing DensePolyNN\n",
      "05:44:10 INFO: Using GPU\n",
      "05:44:10 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.68it/s]\n",
      "05:44:14 INFO: Generating predictions and calculating accuracy\n",
      "05:44:14 INFO: DensePolyNN: accuracy=0.4913\n",
      "05:44:14 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 4/20 ####\n",
      "\n",
      "05:44:14 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:44:14 INFO: Using {'nr_linear_layers': 14, 'nr_conv_layers': 1}\n",
      "05:44:14 INFO: Constructing DensePolyNN\n",
      "05:44:14 INFO: Using GPU\n",
      "05:44:14 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.53it/s]\n",
      "05:44:18 INFO: Generating predictions and calculating accuracy\n",
      "05:44:18 INFO: DensePolyNN: accuracy=0.4549\n",
      "05:44:18 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 5/20 ####\n",
      "\n",
      "05:44:18 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:44:18 INFO: Using {'nr_linear_layers': 14, 'nr_conv_layers': 2}\n",
      "05:44:18 INFO: Constructing DensePolyNN\n",
      "05:44:18 INFO: Using GPU\n",
      "05:44:18 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.51it/s]\n",
      "05:44:22 INFO: Generating predictions and calculating accuracy\n",
      "05:44:22 INFO: DensePolyNN: accuracy=0.527\n",
      "05:44:22 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 6/20 ####\n",
      "\n",
      "05:44:22 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:44:22 INFO: Using {'nr_linear_layers': 14, 'nr_conv_layers': 2}\n",
      "05:44:22 INFO: Constructing DensePolyNN\n",
      "05:44:22 INFO: Using GPU\n",
      "05:44:22 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.44it/s]\n",
      "05:44:27 INFO: Generating predictions and calculating accuracy\n",
      "05:44:27 INFO: DensePolyNN: accuracy=0.4981\n",
      "05:44:27 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 7/20 ####\n",
      "\n",
      "05:44:27 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:44:27 INFO: Using {'nr_linear_layers': 14, 'nr_conv_layers': 3}\n",
      "05:44:27 INFO: Constructing DensePolyNN\n",
      "05:44:27 INFO: Using GPU\n",
      "05:44:27 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.42it/s]\n",
      "05:44:31 INFO: Generating predictions and calculating accuracy\n",
      "05:44:31 INFO: DensePolyNN: accuracy=0.4571\n",
      "05:44:31 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 8/20 ####\n",
      "\n",
      "05:44:31 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:44:31 INFO: Using {'nr_linear_layers': 13, 'nr_conv_layers': 3}\n",
      "05:44:31 INFO: Constructing DensePolyNN\n",
      "05:44:31 INFO: Using GPU\n",
      "05:44:31 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.48it/s]\n",
      "05:44:35 INFO: Generating predictions and calculating accuracy\n",
      "05:44:35 INFO: DensePolyNN: accuracy=0.5007\n",
      "05:44:35 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 9/20 ####\n",
      "\n",
      "05:44:35 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:44:35 INFO: Using {'nr_linear_layers': 13, 'nr_conv_layers': 4}\n",
      "05:44:35 INFO: Constructing DensePolyNN\n",
      "05:44:35 INFO: Using GPU\n",
      "05:44:36 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.53it/s]\n",
      "05:44:39 INFO: Generating predictions and calculating accuracy\n",
      "05:44:39 INFO: DensePolyNN: accuracy=0.4281\n",
      "05:44:40 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 10/20 ####\n",
      "\n",
      "05:44:40 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:44:40 INFO: Using {'nr_linear_layers': 14, 'nr_conv_layers': 4}\n",
      "05:44:40 INFO: Constructing DensePolyNN\n",
      "05:44:40 INFO: Using GPU\n",
      "05:44:40 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.37it/s]\n",
      "05:44:44 INFO: Generating predictions and calculating accuracy\n",
      "05:44:44 INFO: DensePolyNN: accuracy=0.4698\n",
      "05:44:44 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 11/20 ####\n",
      "\n",
      "05:44:44 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:44:44 INFO: Using {'nr_linear_layers': 12, 'nr_conv_layers': 3}\n",
      "05:44:44 INFO: Constructing DensePolyNN\n",
      "05:44:44 INFO: Using GPU\n",
      "05:44:44 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.68it/s]\n",
      "05:44:48 INFO: Generating predictions and calculating accuracy\n",
      "05:44:48 INFO: DensePolyNN: accuracy=0.461\n",
      "05:44:48 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 12/20 ####\n",
      "\n",
      "05:44:48 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:44:48 INFO: Using {'nr_linear_layers': 12, 'nr_conv_layers': 2}\n",
      "05:44:48 INFO: Constructing DensePolyNN\n",
      "05:44:48 INFO: Using GPU\n",
      "05:44:48 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.74it/s]\n",
      "05:44:52 INFO: Generating predictions and calculating accuracy\n",
      "05:44:52 INFO: DensePolyNN: accuracy=0.4966\n",
      "05:44:52 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 13/20 ####\n",
      "\n",
      "05:44:52 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:44:52 INFO: Using {'nr_linear_layers': 12, 'nr_conv_layers': 1}\n",
      "05:44:52 INFO: Constructing DensePolyNN\n",
      "05:44:52 INFO: Using GPU\n",
      "05:44:52 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.77it/s]\n",
      "05:44:56 INFO: Generating predictions and calculating accuracy\n",
      "05:44:56 INFO: DensePolyNN: accuracy=0.4891\n",
      "05:44:56 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 14/20 ####\n",
      "\n",
      "05:44:56 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:44:56 INFO: Using {'nr_linear_layers': 15, 'nr_conv_layers': 2}\n",
      "05:44:56 INFO: Constructing DensePolyNN\n",
      "05:44:56 INFO: Using GPU\n",
      "05:44:56 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.38it/s]\n",
      "05:45:00 INFO: Generating predictions and calculating accuracy\n",
      "05:45:00 INFO: DensePolyNN: accuracy=0.4613\n",
      "05:45:00 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 15/20 ####\n",
      "\n",
      "05:45:00 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:45:00 INFO: Using {'nr_linear_layers': 15, 'nr_conv_layers': 1}\n",
      "05:45:00 INFO: Constructing DensePolyNN\n",
      "05:45:00 INFO: Using GPU\n",
      "05:45:00 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.42it/s]\n",
      "05:45:05 INFO: Generating predictions and calculating accuracy\n",
      "05:45:05 INFO: DensePolyNN: accuracy=0.4531\n",
      "05:45:05 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 16/20 ####\n",
      "\n",
      "05:45:05 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:45:05 INFO: Using {'nr_linear_layers': 11, 'nr_conv_layers': 2}\n",
      "05:45:05 INFO: Constructing DensePolyNN\n",
      "05:45:05 INFO: Using GPU\n",
      "05:45:05 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.92it/s]\n",
      "05:45:08 INFO: Generating predictions and calculating accuracy\n",
      "05:45:08 INFO: DensePolyNN: accuracy=0.5052\n",
      "05:45:08 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 17/20 ####\n",
      "\n",
      "05:45:08 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:45:09 INFO: Using {'nr_linear_layers': 11, 'nr_conv_layers': 1}\n",
      "05:45:09 INFO: Constructing DensePolyNN\n",
      "05:45:09 INFO: Using GPU\n",
      "05:45:09 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.81it/s]\n",
      "05:45:12 INFO: Generating predictions and calculating accuracy\n",
      "05:45:12 INFO: DensePolyNN: accuracy=0.4967\n",
      "05:45:12 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 18/20 ####\n",
      "\n",
      "05:45:12 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:45:12 INFO: Using {'nr_linear_layers': 10, 'nr_conv_layers': 1}\n",
      "05:45:12 INFO: Constructing DensePolyNN\n",
      "05:45:12 INFO: Using GPU\n",
      "05:45:12 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:03<00:00,  3.07it/s]\n",
      "05:45:16 INFO: Generating predictions and calculating accuracy\n",
      "05:45:16 INFO: DensePolyNN: accuracy=0.4873\n",
      "05:45:16 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 19/20 ####\n",
      "\n",
      "05:45:16 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:45:16 INFO: Using {'nr_linear_layers': 10, 'nr_conv_layers': 2}\n",
      "05:45:16 INFO: Constructing DensePolyNN\n",
      "05:45:16 INFO: Using GPU\n",
      "05:45:16 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:03<00:00,  3.02it/s]\n",
      "05:45:19 INFO: Generating predictions and calculating accuracy\n",
      "05:45:19 INFO: DensePolyNN: accuracy=0.4893\n",
      "05:45:19 INFO: \n",
      "\n",
      "#### RUNNING EXPERIMENT 20/20 ####\n",
      "\n",
      "05:45:19 INFO: Applying the bayesian strategy with probability of improvement...\n",
      "05:45:20 INFO: Using {'nr_linear_layers': 11, 'nr_conv_layers': 3}\n",
      "05:45:20 INFO: Constructing DensePolyNN\n",
      "05:45:20 INFO: Using GPU\n",
      "05:45:20 INFO: Training the model\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.81it/s]\n",
      "05:45:23 INFO: Generating predictions and calculating accuracy\n",
      "05:45:23 INFO: DensePolyNN: accuracy=0.515\n"
     ]
    },
    {
     "data": {
      "text/plain": "defaultdict(list,\n            {'DensePolyNN': [0.4735,\n              0.5165,\n              0.4913,\n              0.4549,\n              0.527,\n              0.4981,\n              0.4571,\n              0.5007,\n              0.4281,\n              0.4698,\n              0.461,\n              0.4966,\n              0.4891,\n              0.4613,\n              0.4531,\n              0.5052,\n              0.4967,\n              0.4873,\n              0.4893,\n              0.515]})"
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "accuracies = defaultdict(list)\n",
    "regression_models = {}\n",
    "for experiment in range(config['experiments']):\n",
    "    logging.info(f'\\n\\n#### RUNNING EXPERIMENT {experiment + 1}/{config[\"experiments\"]} ####\\n')\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        models = []\n",
    "\n",
    "        tag = 'DensePolyNN'\n",
    "        config['factors'] = config['optimization_strategy'](config['factor_bounds'], config['random_function'], tag)\n",
    "        model = train(DensePolyNN, config, batches, tag)\n",
    "        models.append(model)\n",
    "\n",
    "        # tag = 'DenseLinearNN'\n",
    "        # config['factors'] = config['optimization_strategy'](config['factor_bounds'], config['random_function'], tag)\n",
    "        # model = train(DenseLinearNN, config, batches, tag)\n",
    "        # models.append(model)\n",
    "\n",
    "        # config['factors'] = config['optimization_strategy'](config['factor_bounds'], config['random_function'], tag)\n",
    "        # model = train(SimpleCNN, config, batches, 'SimpleCNN')\n",
    "        # models.append(model)\n",
    "\n",
    "    accs = test(models, test_x, test_y)\n",
    "    for tag, acc in accs:\n",
    "        if tag in regression_models:\n",
    "            regression_models[tag]['y'].append(acc)\n",
    "        accuracies[tag].append(acc)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "accuracies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}